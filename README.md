# LocalLLM

Browser-based LLM Notebook mit Pyodide und ONNX - Lokale KI-Inferenz ohne Server-AbhÃ¤ngigkeiten

## ğŸš€ Ãœberblick

LocalLLM ist eine innovative LÃ¶sung, die Large Language Models (LLMs) direkt im Browser ausfÃ¼hrt. Durch die Kombination von **Pyodide** (Python im Browser) und **ONNX Runtime Web** ermÃ¶glicht es lokale KI-Inferenz ohne externe Server oder Cloud-AbhÃ¤ngigkeiten.

## âœ¨ Features

- **ğŸŒ Browser-native**: LÃ¤uft vollstÃ¤ndig im Browser ohne Backend-Server
- **ğŸ”’ Privacy-First**: Alle Daten bleiben auf Ihrem GerÃ¤t
- **âš¡ Schnell**: Keine Netzwerk-Latenz durch lokale Verarbeitung
- **ğŸ“± Offline-fÃ¤hig**: Funktioniert ohne Internetverbindung
- **ğŸ Python-Integration**: Volle Python-Umgebung via Pyodide
- **ğŸ“Š Notebook-Interface**: Jupyter-Ã¤hnliche Entwicklungsumgebung
- **ğŸ¤– ONNX-Modelle**: UnterstÃ¼tzung fÃ¼r optimierte ONNX-Modelle
- **ğŸ›ï¸ Interaktiv**: Echtzeit-Code-AusfÃ¼hrung und -Visualisierung

## ğŸ—ï¸ Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Browser                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚     Frontend     â”‚  â”‚     Pyodide     â”‚              â”‚
â”‚  â”‚   (JavaScript)   â”‚  â”‚   (Python)      â”‚              â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚              â”‚
â”‚  â”‚ â€¢ UI Management  â”‚  â”‚ â€¢ Code Executionâ”‚              â”‚
â”‚  â”‚ â€¢ Cell Rendering â”‚  â”‚ â€¢ Data Science  â”‚              â”‚
â”‚  â”‚ â€¢ File I/O      â”‚  â”‚ â€¢ NumPy/Pandas â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚           â”‚                     â”‚                        â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                     â”‚                                    â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚           â”‚  ONNX Runtime   â”‚                           â”‚
â”‚           â”‚     Web         â”‚                           â”‚
â”‚           â”‚                 â”‚                           â”‚
â”‚           â”‚ â€¢ Model Loading â”‚                           â”‚
â”‚           â”‚ â€¢ Inference     â”‚                           â”‚
â”‚           â”‚ â€¢ WebGL/WASM    â”‚                           â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Installation & Setup

### Voraussetzungen

- Python 3.10+
- uv (Package Manager)
- Moderner Browser (Chrome, Firefox, Safari, Edge)

### Schnellstart

1. **Repository klonen**:
```bash
git clone <repository-url>
cd LocalLLM
```

2. **Virtuelle Umgebung erstellen**:
```powershell
# Windows PowerShell
.venv\Scripts\activate
```

3. **Dependencies installieren**:
```bash
uv sync
```

4. **Entwicklungsserver starten**:
```bash
python main.py
```

5. **Browser Ã¶ffnen**: 
   - Automatisch: http://localhost:8000
   - Oder manuell Ã¶ffnen

## ğŸ“– Verwendung

### Erste Schritte

1. **Modell hochladen**: Klicken Sie auf "Upload Model" und wÃ¤hlen Sie eine .onnx-Datei
2. **Zelle erstellen**: Verwenden Sie "+ Cell" oder `Ctrl+Shift+N`
3. **Code ausfÃ¼hren**: DrÃ¼cken Sie `Ctrl+Enter` in einer Code-Zelle
4. **Notebook speichern**: `Ctrl+S` zum Speichern

### TastaturkÃ¼rzel

| KÃ¼rzel | Aktion |
|--------|--------|
| `Ctrl+Enter` | Aktuelle Zelle ausfÃ¼hren |
| `Shift+Enter` | Zelle ausfÃ¼hren und neue Zelle erstellen |
| `Ctrl+S` | Notebook speichern |
| `Ctrl+O` | Notebook Ã¶ffnen |
| `Ctrl+Shift+A` | Alle Zellen ausfÃ¼hren |
| `Ctrl+Shift+N` | Neue Zelle erstellen |

### Beispiel-Code

```python
# Basis Python-FunktionalitÃ¤t
import numpy as np
import matplotlib.pyplot as plt

# Daten generieren
x = np.linspace(0, 10, 100)
y = np.sin(x)

# Plotten
plt.figure(figsize=(10, 6))
plt.plot(x, y)
plt.title('Sinus-Funktion')
plt.show()
```

```python
# LLM-Inferenz (bei geladenem Modell)
prompt = "Die Zukunft der KI ist"
response = generate_text(prompt, max_length=100)
print(f"Eingabe: {prompt}")
print(f"Antwort: {response}")
```

## ğŸ”§ Entwicklung

### Projektstruktur

```
LocalLLM/
â”œâ”€â”€ src/localllm/           # Python-Backend
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core.py            # LLM-KernfunktionalitÃ¤t
â”‚   â”œâ”€â”€ models.py          # Model-Management
â”‚   â”œâ”€â”€ notebook.py        # Notebook-Engine
â”‚   â”œâ”€â”€ server.py          # FastAPI-Server
â”‚   â””â”€â”€ cli.py             # Command-Line Interface
â”œâ”€â”€ frontend/              # Browser-Frontend
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html     # Haupt-HTML
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ css/           # Stylesheets
â”‚       â””â”€â”€ js/            # JavaScript-Module
â”œâ”€â”€ models/                # ONNX-Modelle
â”œâ”€â”€ notebooks/             # Beispiel-Notebooks
â”œâ”€â”€ tests/                 # Tests
â”œâ”€â”€ docs/                  # Dokumentation
â”œâ”€â”€ scripts/               # Build-/Deploy-Skripte
â”œâ”€â”€ main.py               # Haupt-Einstiegspunkt
â””â”€â”€ pyproject.toml        # Projekt-Konfiguration
```

### CLI-Befehle

```bash
# Server starten
localllm server --reload

# Modelle auflisten
localllm list-models

# Modell testen
localllm test-model --model my-model

# Notebook erstellen
localllm create-notebook --example basic

# Notebook ausfÃ¼hren
localllm run-notebook my-notebook.json
```

### API-Endpunkte

- `GET /` - Haupt-Anwendung
- `GET /api/health` - Gesundheitsstatus
- `GET /api/models` - VerfÃ¼gbare Modelle
- `POST /api/models/upload` - Modell hochladen
- `POST /api/models/{name}/load` - Modell laden
- `POST /api/generate` - Text generieren

## ğŸ§ª Modelle

### UnterstÃ¼tzte Formate

- **ONNX**: Optimierte Modelle fÃ¼r Browser-Inferenz
- **Quantisiert**: INT8/INT4-Modelle fÃ¼r bessere Performance
- **WebGL/WASM**: Hardware-beschleunigte Inferenz

### Modell-Konvertierung

```bash
# PyTorch zu ONNX (geplant)
localllm convert-model model.pt model.onnx --format pytorch

# TensorFlow zu ONNX (geplant)
localllm convert-model model.tf model.onnx --format tensorflow
```

### Empfohlene Modelle

- **TinyLLM**: Kleine, schnelle Modelle fÃ¼r den Browser
- **DistilBERT**: Kompakte BERT-Variante
- **GPT-2**: Klassisches generatives Modell
- **T5**: Text-zu-Text-Transfer-Transformer

## ğŸ¯ AnwendungsfÃ¤lle

- **ğŸ”¬ Forschung**: Lokale KI-Experimente ohne Cloud-Kosten
- **ğŸ“š Bildung**: KI-Lernen ohne Datenschutz-Bedenken
- **ğŸ’¼ Business**: Sensitive Datenverarbeitung on-premise
- **ğŸ¨ KreativitÃ¤t**: Interaktive KI-gestÃ¼tzte Inhalte
- **ğŸ› ï¸ Prototyping**: Schnelle KI-Konzept-Validierung

## ğŸ”’ Datenschutz & Sicherheit

- **Keine DatenÃ¼bertragung**: Alle Verarbeitungen bleiben lokal
- **Offline-Betrieb**: Funktioniert ohne Internetverbindung
- **Browser-Sandbox**: AusfÃ¼hrung in sicherer Browser-Umgebung
- **Open Source**: VollstÃ¤ndig nachvollziehbarer Code

## ğŸš€ Roadmap

### Version 0.2.0
- [ ] Erweiterte ONNX-Model-UnterstÃ¼tzung
- [ ] Modell-Konvertierungs-Tools
- [ ] Erweiterte Visualisierungen
- [ ] Plugin-System

### Version 0.3.0
- [ ] Multi-Modal-Modelle (Text + Bild)
- [ ] Collaborative Features
- [ ] Cloud-Sync (optional)
- [ ] Performance-Optimierungen

### Version 1.0.0
- [ ] Produktions-ready
- [ ] Umfassende Dokumentation
- [ ] Beispiel-Gallery
- [ ] Community-Features

## ğŸ¤ Beitragen

Wir freuen uns Ã¼ber BeitrÃ¤ge! Siehe [CONTRIBUTING.md](CONTRIBUTING.md) fÃ¼r Details.

1. **Fork** des Repositories
2. **Feature-Branch** erstellen
3. **Ã„nderungen** committen
4. **Pull Request** Ã¶ffnen

## ğŸ“„ Lizenz

MIT License - siehe [LICENSE](LICENSE) fÃ¼r Details.

## ğŸ™ Danksagungen

- **Pyodide-Team**: Python im Browser ermÃ¶glicht
- **ONNX-Community**: Standardisierte Modell-Formate
- **Hugging Face**: KI-Model-Ecosystem
- **FastAPI**: Moderne Python-Web-Frameworks

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/frank/localllm/issues)
- **Diskussionen**: [GitHub Discussions](https://github.com/frank/localllm/discussions)
- **Email**: support@localllm.dev

---

**LocalLLM** - Bringing AI to your browser, keeping your data with you. ğŸš€ğŸ”’
