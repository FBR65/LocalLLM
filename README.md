# LocalLLM Desktop

Professionelle Desktop-Anwendung fÃ¼r lokale KI-Inferenz mit ONNX-Modellen - Sicherheit und Datenschutz durch lokale Verarbeitung

## ğŸš€ Ãœberblick

LocalLLM Desktop ist eine innovative Desktop-Anwendung, die Large Language Models (LLMs) lokal auf Ihrem Computer ausfÃ¼hrt. Entwickelt mit **Tauri 2.x** und **React**, bietet es eine professionelle BenutzeroberflÃ¤che fÃ¼r KI-gestÃ¼tzte Dokumentenanalyse, Chat-Funktionen und PST-Datei-Exploration - alles ohne externe Server oder Cloud-AbhÃ¤ngigkeiten.

## âœ¨ Features

- **ğŸ–¥ï¸ Desktop-Native**: Professionelle Tauri-basierte Desktop-Anwendung
- **ğŸ”’ Privacy-First**: Alle Daten bleiben auf Ihrem lokalen Computer
- **âš¡ Schnell**: Direkte ONNX-Inferenz ohne Netzwerk-Latenz
- **ğŸ“± Moderne UI**: Professionelles weiÃŸes Design mit React 19
- **ğŸ¤– ONNX-Modelle**: UnterstÃ¼tzung fÃ¼r Llama, Phi, Gemma und BGE-Modelle
- **ï¿½ Dokumentenanalyse**: KI-gestÃ¼tzte Analyse von Textdokumenten
- **ï¿½ Intelligenter Chat**: Lokaler Chat-Bot ohne Emojis, professionell
- **ğŸ“§ PST-Explorer**: Outlook PST-Dateien durchsuchen und analysieren
- **ğŸ›ï¸ Scrollbare Interfaces**: Alle Komponenten mit optimiertem Scrolling

## ğŸ—ï¸ Architektur

```mermaid
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Tauri Desktop App                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   React Frontend â”‚  â”‚   Rust Backend   â”‚              â”‚
â”‚  â”‚  (TypeScript)    â”‚  â”‚    (Tauri)      â”‚              â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚              â”‚
â”‚  â”‚ â€¢ UI Components  â”‚  â”‚ â€¢ File System   â”‚              â”‚
â”‚  â”‚ â€¢ State Mgmt     â”‚  â”‚ â€¢ ONNX Runtime  â”‚              â”‚
â”‚  â”‚ â€¢ Professional  â”‚  â”‚ â€¢ Python Bridge â”‚              â”‚
â”‚  â”‚   White Design   â”‚  â”‚ â€¢ API Layer     â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚           â”‚                     â”‚                        â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                     â”‚                                    â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚           â”‚  ONNX Models    â”‚                           â”‚
â”‚           â”‚   (Local)       â”‚                           â”‚
â”‚           â”‚                 â”‚                           â”‚
â”‚           â”‚ â€¢ Llama 3.2 3B  â”‚                           â”‚
â”‚           â”‚ â€¢ Phi-4 Mini    â”‚                           â”‚
â”‚           â”‚ â€¢ Gemma 3 1B    â”‚                           â”‚
â”‚           â”‚ â€¢ BGE-M3        â”‚                           â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Installation & Setup

### Voraussetzungen

- **Rust** (fÃ¼r Tauri-Entwicklung)
- **Node.js 18+** (fÃ¼r React Frontend)
- **Moderner Desktop** (Windows, macOS, Linux)

### Schnellstart

1. **Repository klonen**:

```bash
git clone <repository-url>
cd LocalLLM
```

2. **Entwicklungsumgebung starten**:

```bash
npm run tauri dev
```

3. **Desktop-App Ã¶ffnet sich automatisch**

### Produktions-Build

```bash
npm run tauri build
```

## ğŸ“– Verwendung

### Professionelle Desktop-Anwendung

Die LocalLLM Desktop-App bietet vier Hauptbereiche:

1. **ğŸ“„ Dokumente**: Lokale Dokumentenanalyse mit KI
   - Dokumente hochladen und analysieren
   - KI-gestÃ¼tzte Zusammenfassungen
   - Scrollbare 3-Panel-Ansicht

2. **ğŸ’¬ Chat**: Professioneller Chat-Bot
   - Lokale ONNX-Modell-Inferenz
   - Saubere, emoji-freie OberflÃ¤che
   - Scrollbare GesprÃ¤chshistorie

3. **ğŸ“§ PST Explorer**: Outlook-Dateien durchsuchen
   - PST-Ordner auswÃ¤hlen (funktional)
   - Email-Listen durchsuchen
   - Manuelle Pfad-Eingabe als Fallback

4. **ğŸ¤– Modelle**: ONNX-Model-Management
   - VerfÃ¼gbare Modelle anzeigen
   - Scrollbare Model-Liste
   - Performance-Informationen

### TastaturkÃ¼rzel

| KÃ¼rzel | Aktion |
|--------|--------|
| `Ctrl+1` | Dokumente-Ansicht |
| `Ctrl+2` | Chat-Ansicht |
| `Ctrl+3` | PST-Explorer |
| `Ctrl+4` | Modelle-Ansicht |
| `Ctrl+S` | Einstellungen speichern |
| `F5` | Aktualisieren |

### VerfÃ¼gbare ONNX-Modelle

```yaml
Modelle:
  - Llama 3.2 3B: Vielseitiges Sprachmodell
  - Phi-4 Mini: Kompaktes, schnelles Modell  
  - Gemma 3 1B: Effizienter Google-Transformer
  - BGE-M3: Hochwertige Text-Embeddings
```

## ğŸ”§ Entwicklung

### Projektstruktur

```text
LocalLLM/
â”œâ”€â”€ src/                   # React Frontend (TypeScript)
â”‚   â”œâ”€â”€ components/        # UI-Komponenten
â”‚   â”‚   â”œâ”€â”€ chat/         # Chat-Interface
â”‚   â”‚   â”œâ”€â”€ document/     # Dokumenten-Viewer
â”‚   â”‚   â”œâ”€â”€ pst/          # PST-Explorer
â”‚   â”‚   â””â”€â”€ models/       # Model-Manager
â”‚   â”œâ”€â”€ App.tsx           # Haupt-Anwendung
â”‚   â””â”€â”€ main.tsx          # React-Einstiegspunkt
â”œâ”€â”€ src-tauri/            # Rust Backend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.rs       # Tauri-Hauptlogik
â”‚   â”‚   â””â”€â”€ lib.rs        # API-Funktionen
â”‚   â””â”€â”€ Cargo.toml        # Rust-Dependencies
â”œâ”€â”€ src/localllm/         # Python-Backend
â”‚   â”œâ”€â”€ core.py           # KI-Kernfunktionen
â”‚   â”œâ”€â”€ models.py         # ONNX-Model-Handling
â”‚   â””â”€â”€ server.py         # FastAPI-Backend
â”œâ”€â”€ models/               # ONNX-Modell-Dateien
â”œâ”€â”€ documents/            # Dokument-Speicher
â”œâ”€â”€ package.json          # Node.js-Dependencies
â””â”€â”€ tauri.conf.json       # Tauri-Konfiguration
```

### Entwicklungsbefehle

```bash
# Entwicklungsserver starten
npm run tauri dev

# Frontend separat entwickeln
npm run dev

# Production Build erstellen
npm run tauri build

# Tests ausfÃ¼hren
npm test

# Rust-Backend einzeln kompilieren
cd src-tauri && cargo build
```

### API-Integration

- **Tauri Commands**: Rust-zu-Frontend-Kommunikation
- **Python Bridge**: ONNX-Modell-Integration
- **File System**: Sichere lokale Dateizugriffe
- **IPC**: Inter-Process-Communication zwischen Frontend/Backend

## ğŸ§ª ONNX-Modelle

### UnterstÃ¼tzte Modelle

- **Llama 3.2 3B**: Vielseitiges Sprachmodell von Meta
- **Phi-4 Mini**: Kompaktes Microsoft-Modell fÃ¼r schnelle Inferenz
- **Gemma 3 1B**: Effizienter Google-Transformer
- **BGE-M3**: Hochwertige Text-Embeddings fÃ¼r Dokumenten-Suche

### Model-Performance

| Modell | GrÃ¶ÃŸe | RAM | Inferenz-Zeit | Anwendung |
|--------|-------|-----|---------------|-----------|
| Llama 3.2 3B | ~6 GB | 8 GB | ~500ms | Allgemeine Konversation |
| Phi-4 Mini | ~2 GB | 4 GB | ~200ms | Schnelle Antworten |
| Gemma 3 1B | ~2 GB | 3 GB | ~150ms | PrÃ¤zise Aufgaben |
| BGE-M3 | ~1 GB | 2 GB | ~50ms | Text-Embeddings |

### Model-Integration

Die Modelle werden automatisch erkannt und in der professionellen UI angezeigt. Jedes Modell zeigt:

- **Performance-Badge**: Geschwindigkeits-Klassifizierung
- **Speicheranforderungen**: RAM-Bedarf
- **Anwendungsbereich**: Empfohlene Nutzung
- **Status**: Geladen/VerfÃ¼gbar/Download erforderlich

## ğŸ¯ AnwendungsfÃ¤lle

- **ğŸ¢ Business Intelligence**: Lokale Dokumentenanalyse ohne Cloud-Risiken
- **ğŸ“š Bildungsbereich**: KI-Lernen ohne Datenschutz-Bedenken
- **ï¿½ Compliance**: DSGVO-konforme KI-Verarbeitung vor Ort
- **ğŸ“§ Email-Analyse**: PST-Dateien durchsuchen und verstehen
- **ğŸ› ï¸ Rapid Prototyping**: Schnelle KI-Konzept-Validierung
- **ğŸ” Forschung**: Lokale Experimente ohne externe AbhÃ¤ngigkeiten

## ğŸ”’ Datenschutz & Sicherheit

- **Zero Cloud**: Alle Daten bleiben auf Ihrem lokalen Computer
- **Offline-Betrieb**: Keine Internetverbindung erforderlich
- **Tauri-Sandbox**: Sichere Desktop-App-Umgebung
- **Open Source**: VollstÃ¤ndig transparenter und auditierbare Code
- **DSGVO-Ready**: Keine DatenÃ¼bertragung an Dritte


## ğŸ“„ Lizenz

MIT License - siehe [LICENSE](LICENSE) fÃ¼r Details.

## ğŸ™ Danksagungen

- **Tauri-Team**: Cross-platform Desktop-App-Framework
- **React-Community**: Moderne UI-Entwicklung
- **ONNX-Runtime**: Optimierte KI-Inferenz
- **Rust-Community**: Sichere System-Programmierung
- **Meta, Microsoft, Google**: Bereitstellung der ONNX-Modelle

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/frank/localllm/issues)
- **Diskussionen**: [GitHub Discussions](https://github.com/frank/localllm/discussions)
- **Email**: [support@localllm.de](mailto:support@localllm.de)

---

**LocalLLM Desktop** - Professionelle KI-LÃ¶sungen fÃ¼r Ihren Desktop. Sicher, lokal, datenschutzkonform. ğŸš€ğŸ”’
