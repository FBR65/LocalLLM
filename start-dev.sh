#!/bin/bash
# LocalLLM Development Starter
# ============================

GREEN='\033[0;32m'
YELLOW='\033[1;33m'
CYAN='\033[0;36m'
WHITE='\033[1;37m'
NC='\033[0m'

echo -e "${CYAN}üöÄ LocalLLM wird gestartet...${NC}"

# Arbeitsverzeichnis ermitteln
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

# Ollama Service √ºberpr√ºfen
if ! pgrep -x "ollama" > /dev/null; then
    echo -e "${YELLOW}üîÑ Ollama wird gestartet...${NC}"
    ollama serve &
    sleep 3
    echo -e "${GREEN}‚úÖ Ollama Service gestartet${NC}"
else
    echo -e "${GREEN}‚úÖ Ollama Service l√§uft bereits${NC}"
fi

# Verf√ºgbare Modelle anzeigen
echo -e "\n${CYAN}üìã Verf√ºgbare Modelle:${NC}"
if command -v ollama &> /dev/null; then
    MODELS=$(ollama list 2>/dev/null)
    if [ ! -z "$MODELS" ]; then
        echo -e "${WHITE}$MODELS${NC}"
    else
        echo -e "${YELLOW}‚ö†Ô∏è  Keine Modelle installiert. Verwenden Sie: ollama pull <model-name>${NC}"
    fi
else
    echo -e "${YELLOW}‚ö†Ô∏è  Konnte Modelle nicht auflisten${NC}"
fi

# LocalLLM Development Server starten
echo -e "\n${GREEN}üåê Development Server wird gestartet...${NC}"
echo -e "${CYAN}üìù Hinweis: Vite Dev Server l√§uft auf http://localhost:5173${NC}"
echo -e "${CYAN}ü§ñ Ollama API l√§uft auf http://localhost:11434${NC}"
echo -e "\n${YELLOW}‚èπÔ∏è  Zum Beenden: Ctrl+C dr√ºcken\n${NC}"

npm run dev
